# Text-fixture-Generation

## Introduction

Fixturize is the first fixture-aware automated test generation framework described in the paper "Fixturize: Bridging the Fixture Gap in Test Generation"

Fixturize addresses the critical "Fixture Gap" in unit testing by treating fixture construction as a proactive diagnostic process. It introduces FixtureEval, the first benchmark explicitly designed to measure fixture reasoning capabilities. FixtureEval consists of 600 functions drawn from 20 popular GitHub repositories, complemented by a manually implemented leakage-free subset to prevent data contamination. Each function is precisely annotated with its fixture dependency, enabling a dual evaluation on both classification and generation capability.

Fixturize is open for research purposes, enabling thorough evaluation of LLMs' capabilities in autonomously constructing multi-step fixture environments and executable unit tests.

## Dataset

The experimental evaluation is conducted on FixtureEval, a benchmark specifically designed to address the lack of fixture-aware metrics in existing datasets. As shown in the directory structure, the benchmark comprises three distinct components:

- **FixtureEval<sub>G</sub>** (`dataset/github`): Contains real-world Python functions collected from popular repositories to evaluate performance in realistic scenarios.
- **FixtureEval<sub>L</sub>**  (`dataset/leakage-free`): Consists of functions meticulously constructed by experts. This subset serves as a rigorous safeguard against data leakage, ensuring that LLMs cannot rely on memorized training data.
- **FixtureEval<sub>J</sub>** (`dataset/java`): A Java extension of the benchmark used to evaluate Fixturize's cross-language generalization capability.

## Environmental Requirements

- Python 3.12+ for Fixturize
- To run Pynguin, we use Python 3.10.18, while keeping all other settings unchanged.
- For CoverUp, we adhered to the default settings. See https://github.com/plasma-umass/coverup-eval

## Getting Started

For the Python version, first fill in the corresponding model, base URL, and API in main.py. Then, input the test dataset in the format indicated in the code:

```python
input_folder1 = 'dataSet/fixture-dependent'
input_folder2 = 'dataSet/fixture-independent'
```

After the execution is complete, the final generated test results can be found in the generated_tests_2 folder.

Other folders:  
- `classified_results` displays the results of the classification part.  
- `generated_calls` presents the successfully generated test calls from the Invocation Generation section.  
- `generated_tests` shows the initial test code generated by the LLM.

***

For the Java version, we use **OpenJDK 23.0.2**. The remaining configuration is consistent with ChatTester.

To get started, you need to merge the two parts of the dataset and place the combined file at `dataSet/merged.json`. Additionally, ensure that the relevant dependencies are specified:

```python
# Configure the following variables before use:

# Fill in your OpenAI-compatible API Key (Required)
api_key = 'api-key'

# Fill in the Base URL for your OpenAI-compatible API (Required)
client = OpenAI(api_key=api_key, base_url='base_url')

# Root directory path of the Java projects to be tested (e.g., '/home/user/projects')
PROJECTS_ROOT = Path("/path/to/your/projects")

# Local file system path to the JDK installation (e.g., '/usr/lib/jvm/java-23-openjdk')
JDK_PATH = "/path/to/your/jdk"

# Path to your Maven settings.xml file (e.g., '~/.m2/settings.xml')
MAVEN_SETTINGS = "/path/to/your/settings.xml"

# Path to your Maven local repository (e.g., '~/.m2/repository')
MAVEN_LOCAL_REPO = "/path/to/your/local/repository"

# Maven command to use (e.g., 'mvn', '/opt/maven/bin/mvn', or the full path to mvn.cmd on Windows)
MAVEN_CMD = "mvn"

# The LLM model name to use for generation (e.g., 'gpt-4', 'claude-3-sonnet')
MODEL = "your-model-name"
```

For the contents of each folder, please refer to the Python section description above.
